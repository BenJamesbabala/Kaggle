{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightFM application to Kaggle Ponpare challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note to self :\n",
    "use --ip=localhost option when launching notebook to show the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure workspace\n",
    "\n",
    "First setup a local repository in which to clone the Kaggle scripts and download the data.  \n",
    "Example: \n",
    "- mkdir Kaggle && cd Kaggle \n",
    "- git clone  https://github.com/tdeboissiere/Kaggle.git\n",
    "- Download the [data](https://www.kaggle.com/c/coupon-purchase-prediction/data)\n",
    "- Copy the data to Kaggle/Ponpare/Data/Data_japanese and unzip\n",
    "\n",
    "Once this is done, we run a series of preprocessing steps :\n",
    "- Translate the data to English \n",
    "- Do a bit of data cleaning \n",
    "- Create validation sets\n",
    "\n",
    "To this end, we first move to the Ponpare_utilities directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/irfulx204/mnt/tmain/Desktop/Notebook/Kaggle/Ponpare/Ponpare_utilities\n"
     ]
    }
   ],
   "source": [
    "cd Ponpare_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import create_validation as val\n",
    "import preprocessing_submission as prep_sub\n",
    "import preprocessing_validation as prep_val\n",
    "import translate as tr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import cPickle as pickle\n",
    "from datetime import date\n",
    "import calendar\n",
    "import scipy.io as spi\n",
    "import scipy.sparse as sps\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are now being translated, this takes a bit of time.  \n",
    "Once this is done, we check the files have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(\n",
    "    os.path.isfile(\"../Data/Data_translated/coupon_detail_train_translated.csv\"))\n",
    "assert(\n",
    "    os.path.isfile(\"../Data/Data_translated/coupon_visit_train_translated.csv\"))\n",
    "assert(os.path.isfile(\"../Data/Data_translated/coupon_list_train_translated.csv\"))\n",
    "assert(os.path.isfile(\"../Data/Data_translated/coupon_list_test_translated.csv\"))\n",
    "assert(os.path.isfile(\"../Data/Data_translated/user_list_translated.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No assertion error, all good !\n",
    "Let us check the translation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              REG_DATE SEX_ID  AGE WITHDRAW_DATE PREF_NAME  \\\n",
      "0  2012-03-28 14:14:18      f   25           NaN   unknown   \n",
      "1  2011-05-18 00:41:48      f   34           NaN     tokyo   \n",
      "2  2011-06-13 16:36:58      m   41           NaN     aichi   \n",
      "3  2012-02-08 12:56:15      m   25           NaN   unknown   \n",
      "4  2011-05-22 23:43:56      m   62           NaN  kanagawa   \n",
      "\n",
      "                       USER_ID_hash  \n",
      "0  d9dca3cb44bab12ba313eaa681f663eb  \n",
      "1  560574a339f1b25e57b0221e486907ed  \n",
      "2  e66ae91b978b3229f8fd858c80615b73  \n",
      "3  43fc18f32eafb05713ec02935e2c2825  \n",
      "4  dc6df8aa860f8db0d710ce9d4839840f  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/Data_translated/user_list_translated.csv\")\n",
    "print df.iloc[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the translation worked.  \n",
    "\"unknown\" filed is for users for which we do not know PREF_NAME.\n",
    "\n",
    "We now create a validation set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We pick the last week of training data (dubbed week52) as our test week\n",
    "# All coupons emitted during this week become our new test coupons\n",
    "val.create_validation_set([2012, 06, 17], [2012, 06, 23], \"week52\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the files have been created properly :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(os.path.isfile(\n",
    "    \"../Data/Validation/week52/coupon_detail_train_validation_week52.csv\"))\n",
    "assert(os.path.isfile(\n",
    "    \"../Data/Validation/week52/coupon_visit_train_validation_week52.csv\"))\n",
    "assert(os.path.isfile(\n",
    "    \"../Data/Validation/week52/coupon_list_train_validation_week52.csv\"))\n",
    "assert(os.path.isfile(\n",
    "    \"../Data/Validation/week52/coupon_list_test_validation_week52.csv\"))\n",
    "assert(\n",
    "    os.path.isfile(\"../Data/Validation/week52/user_list_validation_week52.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good !\n",
    "We can now move on to the more interesting part : building a recommender with lightFM.\n",
    "\n",
    "We start by creating the sparse matrices which we will use to store implicit feedback information and user/item attributes.\n",
    "\n",
    "Let us write down 3 functions for that :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of user feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_user_feature_matrix(week_ID):\n",
    "    \"\"\" Build user feature matrix\n",
    "    (feat = AGE, SEX_ID, these feat are then binarized)\n",
    "\n",
    "    arg : week_ID (str) validation week\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print \"Creating user_feature matrix for LightFM\"\n",
    "\n",
    "    def age_function(age, age_low=0, age_up=100):\n",
    "        \"\"\"Binarize age in age slices\"\"\"\n",
    "        \n",
    "        if age_low <= age < age_up:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def format_reg_date(row):\n",
    "        \"\"\"Format reg date to \"year-month\" \"\"\"\n",
    "        \n",
    "        row = row.split(\" \")\n",
    "        row = row[0].split(\"-\")\n",
    "        reg_date = row[0]  # + row[1]\n",
    "        return reg_date\n",
    "\n",
    "    ulist = pd.read_csv(\n",
    "        \"../Data/Validation/%s/user_list_validation_%s.csv\" %\n",
    "        (week_ID, week_ID))\n",
    "\n",
    "    # Format REG_DATE\n",
    "    ulist[\"REG_DATE\"] = ulist[\"REG_DATE\"].apply(format_reg_date)\n",
    "\n",
    "    # Segment the age\n",
    "    ulist[\"0to30\"] = ulist[\"AGE\"].apply(age_function, age_low=0, age_up=30)\n",
    "    ulist[\"30to50\"] = ulist[\"AGE\"].apply(\n",
    "        age_function,\n",
    "        age_low=30,\n",
    "        age_up=50)\n",
    "    ulist[\"50to100\"] = ulist[\"AGE\"].apply(\n",
    "        age_function,\n",
    "        age_low=50,\n",
    "        age_up=100)\n",
    "\n",
    "    list_age_bin = [col for col in ulist.columns.values if \"to\" in col]\n",
    "    ulist = ulist[[\"USER_ID_hash\",\n",
    "                   \"PREF_NAME\",\n",
    "                   \"SEX_ID\",\n",
    "                   \"REG_DATE\"] + list_age_bin]\n",
    "\n",
    "    ulist = ulist.T.to_dict().values()\n",
    "    vec = DictVectorizer(sparse=True)\n",
    "    ulist = vec.fit_transform(ulist)\n",
    "    # ulist is in csr format, make sure the type is int\n",
    "    ulist = sps.csr_matrix(ulist, dtype=np.int32)\n",
    "\n",
    "    # Save the matrix. They are already in csr format\n",
    "    spi.mmwrite(\n",
    "        \"../Data/Validation/%s/user_feat_mtrx_%s.mtx\" %\n",
    "        (week_ID, week_ID), ulist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of item feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_item_feature_matrix(week_ID):\n",
    "    \"\"\" Build item feature matrix\n",
    "\n",
    "    arg : week_ID (str) validation week\n",
    "    \"\"\"\n",
    "\n",
    "    print \"Creating item_feature matrix for LightFM\"\n",
    "\n",
    "    def binarize_function(val, val_low=0, val_up=100):\n",
    "        \"\"\"Function to binarize a given column in slices\n",
    "        \"\"\"\n",
    "        if val_low <= val < val_up:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Utility to convert a date to the day of the week\n",
    "    #(indexed by i in [0,1,..6])\n",
    "    def get_day_of_week(row):\n",
    "        \"\"\"Convert to unix time. Neglect time of the day\n",
    "        \"\"\"\n",
    "        row = row.split(\" \")\n",
    "        row = row[0].split(\"-\")\n",
    "        y, m, d = int(row[0]), int(row[1]), int(row[2])\n",
    "        return date(y, m, d).weekday()\n",
    "\n",
    "    # Load coupon data\n",
    "    cpltr = pd.read_csv(\n",
    "        \"../Data/Validation/%s/coupon_list_train_validation_%s.csv\" %\n",
    "        (week_ID, week_ID))\n",
    "    cplte = pd.read_csv(\n",
    "        \"../Data/Validation/%s/coupon_list_test_validation_%s.csv\" %\n",
    "        (week_ID, week_ID))\n",
    "\n",
    "    cplte[\"DISPFROM_day\"] = cplte[\"DISPFROM\"].apply(get_day_of_week)\n",
    "    cpltr[\"DISPFROM_day\"] = cpltr[\"DISPFROM\"].apply(get_day_of_week)\n",
    "    cplte[\"DISPEND_day\"] = cplte[\"DISPEND\"].apply(get_day_of_week)\n",
    "    cpltr[\"DISPEND_day\"] = cpltr[\"DISPEND\"].apply(get_day_of_week)\n",
    "\n",
    "    cpltr[\"PRICE_0to50\"] = cpltr[\"PRICE_RATE\"].apply(\n",
    "        binarize_function,\n",
    "        val_low=0,\n",
    "        val_up=30)\n",
    "    cpltr[\"PRICE_50to70\"] = cpltr[\"PRICE_RATE\"].apply(\n",
    "        binarize_function,\n",
    "        val_low=50,\n",
    "        val_up=70)\n",
    "    cpltr[\"PRICE_70to100\"] = cpltr[\"PRICE_RATE\"].apply(\n",
    "        binarize_function,\n",
    "        val_low=70,\n",
    "        val_up=100)\n",
    "\n",
    "    cplte[\"PRICE_0to50\"] = cplte[\"PRICE_RATE\"].apply(\n",
    "        binarize_function,\n",
    "        val_low=0,\n",
    "        val_up=30)\n",
    "    cplte[\"PRICE_50to70\"] = cplte[\"PRICE_RATE\"].apply(\n",
    "        binarize_function,\n",
    "        val_low=50,\n",
    "        val_up=51)\n",
    "    cplte[\"PRICE_70to100\"] = cplte[\"PRICE_RATE\"].apply(\n",
    "        binarize_function,\n",
    "        val_low=51,\n",
    "        val_up=100)\n",
    "\n",
    "    list_quant_name = [0, 20, 40, 60, 80, 100]\n",
    "    quant_step = list_quant_name[1] - list_quant_name[0]\n",
    "\n",
    "    list_prices = cpltr[\"CATALOG_PRICE\"].values\n",
    "    list_quant = [np.percentile(list_prices, quant)\n",
    "                  for quant in list_quant_name]\n",
    "\n",
    "    for index, (quant_name, quant) in enumerate(zip(list_quant_name, list_quant)):\n",
    "        if index > 0:\n",
    "            cpltr[\"CAT_%sto%s\" % (\n",
    "                quant_name - quant_step,\n",
    "                quant_name)] = cpltr[\"CATALOG_PRICE\"].apply(binarize_function,\n",
    "                                                            val_low=list_quant[\n",
    "                                                                index -\n",
    "                                                                1],\n",
    "                                                            val_up=quant)\n",
    "            cplte[\"CAT_%sto%s\" % (\n",
    "                quant_name - quant_step,\n",
    "                quant_name)] = cplte[\"CATALOG_PRICE\"].apply(binarize_function,\n",
    "                                                            val_low=list_quant[\n",
    "                                                                index -\n",
    "                                                                1],\n",
    "                                                            val_up=quant)\n",
    "\n",
    "    list_prices = cpltr[\"DISCOUNT_PRICE\"].values\n",
    "    list_quant = [np.percentile(list_prices, quant)\n",
    "                  for quant in list_quant_name]\n",
    "    for index, (quant_name, quant) in enumerate(zip(list_quant_name, list_quant)):\n",
    "        if index > 0:\n",
    "            cpltr[\"DIS_%sto%s\" % (\n",
    "                quant_name - quant_step,\n",
    "                quant_name)] = cpltr[\"DISCOUNT_PRICE\"].apply(binarize_function,\n",
    "                                                             val_low=list_quant[\n",
    "                                                                 index -\n",
    "                                                                 1],\n",
    "                                                             val_up=quant)\n",
    "            cplte[\"DIS_%sto%s\" % (\n",
    "                quant_name - quant_step,\n",
    "                quant_name)] = cplte[\"DISCOUNT_PRICE\"].apply(binarize_function,\n",
    "                                                             val_low=list_quant[\n",
    "                                                                 index -\n",
    "                                                                 1],\n",
    "                                                             val_up=quant)\n",
    "\n",
    "    list_col_bin = [col for col in cplte.columns.values if \"to\" in col]\n",
    "\n",
    "    # List of features\n",
    "    list_feat = [\n",
    "        \"GENRE_NAME\", \"large_area_name\", \"small_area_name\", \"VALIDPERIOD\", \"USABLE_DATE_MON\", \"USABLE_DATE_TUE\",\n",
    "        \"USABLE_DATE_WED\", \"USABLE_DATE_THU\", \"USABLE_DATE_FRI\",\n",
    "        \"USABLE_DATE_SAT\", \"USABLE_DATE_SUN\", \"USABLE_DATE_HOLIDAY\",\n",
    "        \"USABLE_DATE_BEFORE_HOLIDAY\"] + list_col_bin\n",
    "\n",
    "    # NA imputation\n",
    "    cplte = cplte.fillna(-1)\n",
    "    cpltr = cpltr.fillna(-1)\n",
    "\n",
    "    list_col_to_str = [\n",
    "        \"PRICE_RATE\",\n",
    "        \"CATALOG_PRICE\",\n",
    "        \"DISCOUNT_PRICE\",\n",
    "        \"DISPFROM_day\",\n",
    "        \"DISPEND_day\",\n",
    "        \"DISPPERIOD\",\n",
    "        \"VALIDPERIOD\"]\n",
    "    cpltr[list_col_to_str] = cpltr[list_col_to_str].astype(str)\n",
    "    cplte[list_col_to_str] = cplte[list_col_to_str].astype(str)\n",
    "\n",
    "    # Reduce dataset to features of interest\n",
    "    cpltr = cpltr[list_feat]\n",
    "    cplte = cplte[list_feat]\n",
    "\n",
    "    list_us = [col for col in list_feat if \"USABLE\" in col]\n",
    "    for col in list_us:\n",
    "        cpltr.loc[cpltr[col] > 0, col] = 1\n",
    "        cpltr.loc[cpltr[col] < 0, col] = 0\n",
    "        cplte.loc[cpltr[col] > 0, col] = 1\n",
    "        cplte.loc[cpltr[col] < 0, col] = 0\n",
    "\n",
    "    # Binarize categorical features\n",
    "    cpltr = cpltr.T.to_dict().values()\n",
    "    vec = DictVectorizer(sparse=True)\n",
    "    cpltr = vec.fit_transform(cpltr)\n",
    "    cplte = vec.transform(cplte.T.to_dict().values())\n",
    "\n",
    "    cplte = sps.csr_matrix(cplte, dtype=np.int32)\n",
    "    cpltr = sps.csr_matrix(cpltr, dtype=np.int32)\n",
    "\n",
    "    # Save the matrix. They are already in csr format\n",
    "    spi.mmwrite(\n",
    "        \"../Data/Validation/%s/train_item_feat_mtrx_%s.mtx\" %\n",
    "        (week_ID, week_ID), cpltr)\n",
    "    spi.mmwrite(\n",
    "        \"../Data/Validation/%s/test_item_feat_mtrx_%s.mtx\" %\n",
    "        (week_ID, week_ID), cplte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of implicit feed back user/item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_user_item_mtrx(week_ID):\n",
    "    \"\"\" Build user item matrix (for test and train datasets)\n",
    "    (sparse matrix, Mui[u,i] = 1 if user u has purchase item i, 0 otherwise)\n",
    "\n",
    "    arg : week_ID (str) validation week\n",
    "    \"\"\"\n",
    "\n",
    "    print \"Creating user_item matrix for LightFM\"\n",
    "\n",
    "    # For now, only consider the detail dataset\n",
    "    cpdtr = pd.read_csv(\n",
    "        \"../Data/Validation/%s/coupon_detail_train_validation_%s.csv\" %\n",
    "        (week_ID, week_ID))\n",
    "    cpltr = pd.read_csv(\n",
    "        \"../Data/Validation/%s/coupon_list_train_validation_%s.csv\" %\n",
    "        (week_ID, week_ID))\n",
    "    cplte = pd.read_csv(\n",
    "        \"../Data/Validation/%s/coupon_list_test_validation_%s.csv\" %\n",
    "        (week_ID, week_ID))\n",
    "    ulist = pd.read_csv(\n",
    "        \"../Data/Validation/%s/user_list_validation_%s.csv\" %\n",
    "        (week_ID, week_ID))\n",
    "\n",
    "    # Build a dict with the coupon index in cpltr\n",
    "    d_ci_tr = {}\n",
    "    for i in range(len(cpltr)):\n",
    "        coupon = cpltr[\"COUPON_ID_hash\"].values[i]\n",
    "        d_ci_tr[coupon] = i\n",
    "\n",
    "    # Build a dict with the user index in ulist\n",
    "    d_ui = {}\n",
    "    for i in range(len(ulist)):\n",
    "        user = ulist[\"USER_ID_hash\"].values[i]\n",
    "        d_ui[user] = i\n",
    "\n",
    "    # Build the user x item matrices using scipy lil_matrix\n",
    "    Mui_tr = sps.lil_matrix((len(ulist), len(cpltr)), dtype=np.int8)\n",
    "\n",
    "    # Now fill Mui_tr with the info from cpdtr\n",
    "    for i in range(len(cpdtr)):\n",
    "        sys.stdout.write(\n",
    "            \"\\rProcessing row \" + str(i) + \"/ \" + str(cpdtr.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "        user = cpdtr[\"USER_ID_hash\"].values[i]\n",
    "        coupon = cpdtr[\"COUPON_ID_hash\"].values[i]\n",
    "        ui, ci = d_ui[user], d_ci_tr[coupon]\n",
    "        Mui_tr[ui, ci] = 1\n",
    "    print\n",
    "\n",
    "    # Save the matrix in the COO format\n",
    "    spi.mmwrite(\n",
    "        \"../Data/Validation/%s/user_item_train_mtrx_%s.mtx\" %\n",
    "        (week_ID, week_ID), Mui_tr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inline documentation of these functions is self-explanatory. We basically specify which features we want to take into account for either users or items and we consider an implicit feedback-only case where only purchases are counted as interactions.\n",
    "\n",
    "We can now create these matrices :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build_user_item_mtrx(\"week52\")\n",
    "build_user_feature_matrix(\"week52\")\n",
    "build_item_feature_matrix(\"week52\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, let's check the matrices were created :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "week_ID = \"week52\"\n",
    "assert(\n",
    "    os.path.isfile(\"../Data/Validation/%s/user_item_train_mtrx_%s.mtx\" %\n",
    "                   (week_ID, week_ID)))\n",
    "assert(\n",
    "    os.path.isfile(\"../Data/Validation/%s/train_item_feat_mtrx_%s.mtx\" %\n",
    "                   (week_ID, week_ID)))\n",
    "assert(\n",
    "    os.path.isfile(\"../Data/Validation/%s/test_item_feat_mtrx_%s.mtx\" %\n",
    "                   (week_ID, week_ID)))\n",
    "assert(\n",
    "    os.path.isfile(\"../Data/Validation/%s/user_feat_mtrx_%s.mtx\" %\n",
    "                   (week_ID, week_ID)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright !\n",
    "Let us now load these matrices :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "week_ID = \"week52\"\n",
    "Mui_train = spi.mmread(\n",
    "    \"../Data/Validation/%s/user_item_train_mtrx_%s.mtx\" %\n",
    "    (week_ID, week_ID))\n",
    "uf = spi.mmread(\n",
    "    \"../Data/Validation/%s/user_feat_mtrx_%s.mtx\" %\n",
    "    (week_ID, week_ID))\n",
    "itrf = spi.mmread(\n",
    "    \"../Data/Validation/%s/train_item_feat_mtrx_%s.mtx\" %\n",
    "    (week_ID, week_ID))\n",
    "itef = spi.mmread(\n",
    "    \"../Data/Validation/%s/test_item_feat_mtrx_%s.mtx\" %\n",
    "    (week_ID, week_ID))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import lightfm and our evaluation metric (MAP@10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "import mean_average_precision as mapr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a function to fit a lightFM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_model(week_ID, no_comp, lr, ep):\n",
    "    \"\"\" Fit the lightFM model to all weeks in list_week_ID.\n",
    "    Then print the results for MAPat10\n",
    "\n",
    "    args : week_ID validation test week\n",
    "    no_comp, lr, ep = (int, float, int) number of components, learning rate, number of epochs for lightFM model\n",
    "\n",
    "returns: d_user_pred, list_user, list_coupon\n",
    "list_coupon = list of test coupons\n",
    "list_user = list of user ID\n",
    "d_user_pred : key = user, value = predicted ranking of coupons in list_coupon\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print \"Fit lightfm model for %s\" % week_ID\n",
    "\n",
    "    # Load data\n",
    "    Mui_train = spi.mmread(\n",
    "        \"../Data/Validation/%s/user_item_train_mtrx_%s.mtx\" %\n",
    "        (week_ID, week_ID))\n",
    "    uf = spi.mmread(\n",
    "        \"../Data/Validation/%s/user_feat_mtrx_%s.mtx\" %\n",
    "        (week_ID, week_ID))\n",
    "    itrf = spi.mmread(\n",
    "        \"../Data/Validation/%s/train_item_feat_mtrx_%s.mtx\" %\n",
    "        (week_ID, week_ID))\n",
    "    itef = spi.mmread(\n",
    "        \"../Data/Validation/%s/test_item_feat_mtrx_%s.mtx\" %\n",
    "        (week_ID, week_ID))\n",
    "\n",
    "    # Print shapes as a check\n",
    "    print \"user_features shape: %s,\\nitem train features shape: %s,\\nitem test features shape: %s\" % (uf.shape, itrf.shape, itef.shape)\n",
    "\n",
    "    # Load test coupon  and user lists\n",
    "    cplte = pd.read_csv(\n",
    "        \"../Data/Validation/\" +\n",
    "        week_ID +\n",
    "        \"/coupon_list_test_validation_\" +\n",
    "        week_ID +\n",
    "        \".csv\")\n",
    "    ulist = pd.read_csv(\n",
    "        \"../Data/Validation/\" +\n",
    "        week_ID +\n",
    "        \"/user_list_validation_\" +\n",
    "        week_ID +\n",
    "        \".csv\")\n",
    "    list_coupon = cplte[\"COUPON_ID_hash\"].values\n",
    "    list_user = ulist[\"USER_ID_hash\"].values\n",
    "\n",
    "    # Build model\n",
    "    model = LightFM(no_components=no_comp, learning_rate=lr, loss='warp')\n",
    "    model.fit_partial(\n",
    "        Mui_train,\n",
    "        user_features=uf,\n",
    "        item_features=itrf,\n",
    "        epochs=ep,\n",
    "        num_threads=4,\n",
    "        verbose=True)\n",
    "\n",
    "    test = sps.csr_matrix(\n",
    "        (len(list_user),\n",
    "         len(list_coupon)),\n",
    "        dtype=np.int32)\n",
    "    no_users, no_items = test.shape\n",
    "    pid_array = np.arange(no_items, dtype=np.int32)\n",
    "\n",
    "    # Create and initialise dict to store predictions\n",
    "    d_user_pred = {}\n",
    "    for user in list_user:\n",
    "        d_user_pred[user] = []\n",
    "\n",
    "    # Loop over users and compute predictions\n",
    "    for user_id, row in enumerate(test):\n",
    "        sys.stdout.write(\"\\rProcessing user \" + str(user_id)\n",
    "                         + \"/ \" + str(len(list_user)))\n",
    "        sys.stdout.flush()\n",
    "        uid_array = np.empty(no_items, dtype=np.int32)\n",
    "        uid_array.fill(user_id)\n",
    "        predictions = model.predict(\n",
    "            uid_array,\n",
    "            pid_array,\n",
    "            user_features=uf,\n",
    "            item_features=itef,\n",
    "            num_threads=4)\n",
    "        user = str(list_user[user_id])\n",
    "        d_user_pred[user] = predictions\n",
    "\n",
    "    return d_user_pred, list_user, list_coupon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a scoring function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_lightFM(no_comp, lr, ep):\n",
    "    \"\"\"\n",
    "    Score the lightFM model for mean average precision at k = 10\n",
    "\n",
    "    args = no_comp, lr, ep (int, float, int)\n",
    "    number of components, learning rate, number of epochs for lightFM model\n",
    "    \"\"\"\n",
    "\n",
    "    list_score = []\n",
    "\n",
    "    # Loop over validation weeks\n",
    "    for week_ID in [\"week52\"]:\n",
    "        # Get predictions, manually choose metric and classifier\n",
    "        d_user_pred, list_user_full, list_coupon = fit_model(\n",
    "            week_ID, no_comp, lr, ep)\n",
    "        # Format predictions\n",
    "        for index, user in enumerate(list_user_full):\n",
    "            list_pred = d_user_pred[user]\n",
    "            top_k = np.argsort(-list_pred)[:10]\n",
    "            d_user_pred[user] = list_coupon[top_k]\n",
    "\n",
    "        # Get actual purchase\n",
    "        d_user_purchase = {}\n",
    "        with open(\"../Data/Validation/\" + week_ID + \"/dict_purchase_validation_\" + week_ID + \".pickle\", \"r\") as fp:\n",
    "            d_user_purchase = pickle.load(fp)\n",
    "\n",
    "        # Take care of users who registered during validation test week\n",
    "        for key in d_user_purchase.keys():\n",
    "            try:\n",
    "                d_user_pred[key]\n",
    "            except KeyError:\n",
    "                d_user_pred[key] = []\n",
    "\n",
    "        list_user = d_user_purchase.keys()\n",
    "        list_actual = [d_user_purchase[key] for key in list_user]\n",
    "        list_pred = [d_user_pred[key] for key in list_user]\n",
    "\n",
    "    list_score.append(mapr.mapk(list_actual, list_pred))\n",
    "    print list_score\n",
    "\n",
    "    list_score = np.array(list_score)\n",
    "    print list_score\n",
    "    print str(np.mean(list_score)) + \" +/- \" + str(np.std(list_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate our lightFM model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_comp, lr, ep = 10, 0.01, 10  # 10 components, 0.01 learning rate, 10 epochs\n",
    "score_lightFM(no_comp, lr, ep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightFM application to Kaggle Ponpare challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note to self :\n",
    "use --ip=localhost option when launching notebook to show the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure workspace\n",
    "\n",
    "First setup a local repository in which to clone the Kaggle scripts and download the data.  \n",
    "Example: \n",
    "- mkdir Kaggle && cd Kaggle \n",
    "- git clone  https://github.com/tdeboissiere/Kaggle.git\n",
    "- Download the [data](https://www.kaggle.com/c/coupon-purchase-prediction/data)\n",
    "- Copy the data to Kaggle/Ponpare/Data/Data_japanese and unzip\n",
    "\n",
    "Once this is done, we run a series of preprocessing steps :\n",
    "- Translate the data to English \n",
    "- Do a bit of data cleaning \n",
    "- Create validation sets\n",
    "\n",
    "To this end, we first move to the Ponpare_utilities directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/irfulx204/mnt/tmain/Desktop/Notebook/Kaggle/Ponpare/Ponpare_utilities\n"
     ]
    }
   ],
   "source": [
    "cd Ponpare_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##We then import the relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import create_validation as val\n",
    "import preprocessing_submission as prep_sub\n",
    "import preprocessing_validation as prep_val\n",
    "import translate as tr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn import preprocessing\n",
    "import sys \n",
    "import cPickle as pickle\n",
    "from datetime import date\n",
    "import calendar\n",
    "import scipy.io as spi\n",
    "import scipy.sparse as sps\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are now being translated, this takes a bit of time.  \n",
    "Once this is done, we check the files have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "assert(os.path.isfile(\"../Data/Data_translated/coupon_detail_train_translated.csv\"))\n",
    "assert(os.path.isfile(\"../Data/Data_translated/coupon_visit_train_translated.csv\"))\n",
    "assert(os.path.isfile(\"../Data/Data_translated/coupon_list_train_translated.csv\"))\n",
    "assert(os.path.isfile(\"../Data/Data_translated/coupon_list_test_translated.csv\"))\n",
    "assert(os.path.isfile(\"../Data/Data_translated/user_list_translated.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No assertion error, all good !\n",
    "Let us check the translation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              REG_DATE SEX_ID  AGE WITHDRAW_DATE PREF_NAME  \\\n",
      "0  2012-03-28 14:14:18      f   25           NaN   unknown   \n",
      "1  2011-05-18 00:41:48      f   34           NaN     tokyo   \n",
      "2  2011-06-13 16:36:58      m   41           NaN     aichi   \n",
      "3  2012-02-08 12:56:15      m   25           NaN   unknown   \n",
      "4  2011-05-22 23:43:56      m   62           NaN  kanagawa   \n",
      "\n",
      "                       USER_ID_hash  \n",
      "0  d9dca3cb44bab12ba313eaa681f663eb  \n",
      "1  560574a339f1b25e57b0221e486907ed  \n",
      "2  e66ae91b978b3229f8fd858c80615b73  \n",
      "3  43fc18f32eafb05713ec02935e2c2825  \n",
      "4  dc6df8aa860f8db0d710ce9d4839840f  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../Data/Data_translated/user_list_translated.csv\")\n",
    "print df.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the translation worked.  \n",
    "\"unknown\" filed is for users for which we do not know PREF_NAME.\n",
    "\n",
    "We now create a validation set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We pick the last week of training data (dubbed week52) as our test week\n",
    "# All coupons emitted during this week become our new test coupons\n",
    "val.create_validation_set([2012,06,17], [2012, 06, 23], \"week52\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the files have been created properly :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(os.path.isfile(\"../Data/Validation/week52/coupon_detail_train_validation_week52.csv\"))\n",
    "assert(os.path.isfile(\"../Data/Validation/week52/coupon_visit_train_validation_week52.csv\"))\n",
    "assert(os.path.isfile(\"../Data/Validation/week52/coupon_list_train_validation_week52.csv\"))\n",
    "assert(os.path.isfile(\"../Data/Validation/week52/coupon_list_test_validation_week52.csv\"))\n",
    "assert(os.path.isfile(\"../Data/Validation/week52/user_list_validation_week52.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good !\n",
    "We can now move on to the more interesting part : building a recommender with lightFM.\n",
    "\n",
    "We start by creating the sparse matrices which we will use to store implicit feedback information and user/item attributes.\n",
    "\n",
    "Let us write down 3 functions for that :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of user feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_user_feature_matrix(week_ID):\n",
    "    \"\"\" Build user feature matrix \n",
    "    (feat = AGE, SEX_ID, these feat are then binarized)\n",
    "    \n",
    "    arg : week_ID (str) validation week\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print \"Creating user_feature matrix for LightFM\"\n",
    "\n",
    "    def age_function(age, age_low =0, age_up = 100):\n",
    "        \"\"\"Function to binarize age in age slices\n",
    "        \"\"\"\n",
    "        if age_low<= age < age_up :\n",
    "            return 1 \n",
    "        else :\n",
    "            return 0\n",
    "\n",
    "    def format_reg_date(row):\n",
    "        \"\"\"Format reg date to \"year-month\"\n",
    "        \"\"\"\n",
    "        row = row.split(\" \")\n",
    "        row = row[0].split(\"-\")\n",
    "        reg_date = row[0] #+ row[1]\n",
    "        return reg_date\n",
    "\n",
    "    ulist = pd.read_csv(\"../Data/Validation/%s/user_list_validation_%s.csv\" % (week_ID, week_ID))\n",
    "\n",
    "    #Format REG_DATE\n",
    "    ulist[\"REG_DATE\"] = ulist[\"REG_DATE\"].apply(format_reg_date)\n",
    "\n",
    "    #Segment the age\n",
    "    ulist[\"0to30\"] = ulist[\"AGE\"].apply(age_function, age_low = 0, age_up = 30)\n",
    "    ulist[\"30to50\"] = ulist[\"AGE\"].apply(age_function, age_low = 30, age_up = 50)\n",
    "    ulist[\"50to100\"] = ulist[\"AGE\"].apply(age_function, age_low = 50, age_up = 100)\n",
    "\n",
    "    list_age_bin = [col for col in ulist.columns.values if \"to\" in col]\n",
    "    ulist = ulist[[\"USER_ID_hash\", \"PREF_NAME\", \"SEX_ID\", \"REG_DATE\"] + list_age_bin]\n",
    "    \n",
    "    ulist =  ulist.T.to_dict().values()\n",
    "    vec   = DictVectorizer(sparse = True)\n",
    "    ulist = vec.fit_transform(ulist)    \n",
    "    #ulist is in csr format, make sure the type is int\n",
    "    ulist = sps.csr_matrix(ulist, dtype = np.int32)\n",
    "\n",
    "    #Save the matrix. They are already in csr format\n",
    "    spi.mmwrite(\"../Data/Validation/%s/user_feat_mtrx_%s.mtx\" % (week_ID, week_ID) , ulist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of item feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_item_feature_matrix(week_ID):\n",
    "    \"\"\" Build item feature matrix \n",
    "\n",
    "    arg : week_ID (str) validation week\n",
    "    \"\"\"\n",
    "\n",
    "    print \"Creating item_feature matrix for LightFM\"\n",
    "\n",
    "    def binarize_function(val, val_low =0, val_up = 100):\n",
    "        \"\"\"Function to binarize a given column in slices\n",
    "        \"\"\"\n",
    "        if val_low<= val < val_up :\n",
    "            return 1 \n",
    "        else :\n",
    "            return 0\n",
    "\n",
    "    #Utility to convert a date to the day of the week \n",
    "    #(indexed by i in [0,1,..6])\n",
    "    def get_day_of_week(row):\n",
    "        \"\"\"Convert to unix time. Neglect time of the day\n",
    "        \"\"\"\n",
    "        row = row.split(\" \")\n",
    "        row = row[0].split(\"-\")\n",
    "        y,m,d = int(row[0]), int(row[1]), int(row[2])\n",
    "        return date(y,m,d).weekday()\n",
    "\n",
    "    #Load coupon data\n",
    "    cpltr = pd.read_csv(\"../Data/Validation/%s/coupon_list_train_validation_%s.csv\" % (week_ID, week_ID))\n",
    "    cplte = pd.read_csv(\"../Data/Validation/%s/coupon_list_test_validation_%s.csv\" % (week_ID, week_ID))\n",
    "\n",
    "    cplte[\"DISPFROM_day\"] = cplte[\"DISPFROM\"].apply(get_day_of_week)\n",
    "    cpltr[\"DISPFROM_day\"] = cpltr[\"DISPFROM\"].apply(get_day_of_week)\n",
    "    cplte[\"DISPEND_day\"] = cplte[\"DISPEND\"].apply(get_day_of_week)\n",
    "    cpltr[\"DISPEND_day\"] = cpltr[\"DISPEND\"].apply(get_day_of_week)\n",
    "\n",
    "    cpltr[\"PRICE_0to50\"] = cpltr[\"PRICE_RATE\"].apply(binarize_function, val_low = 0, val_up = 30)\n",
    "    cpltr[\"PRICE_50to70\"] = cpltr[\"PRICE_RATE\"].apply(binarize_function, val_low = 50, val_up = 70)\n",
    "    cpltr[\"PRICE_70to100\"] = cpltr[\"PRICE_RATE\"].apply(binarize_function, val_low = 70, val_up = 100)\n",
    "\n",
    "    cplte[\"PRICE_0to50\"] = cplte[\"PRICE_RATE\"].apply(binarize_function, val_low = 0, val_up = 30)\n",
    "    cplte[\"PRICE_50to70\"] = cplte[\"PRICE_RATE\"].apply(binarize_function, val_low = 50, val_up = 51)\n",
    "    cplte[\"PRICE_70to100\"] = cplte[\"PRICE_RATE\"].apply(binarize_function, val_low = 51, val_up = 100)\n",
    "\n",
    "    list_quant_name = [0, 20, 40, 60, 80, 100]\n",
    "    quant_step = list_quant_name[1] - list_quant_name[0]\n",
    "\n",
    "    list_prices = cpltr[\"CATALOG_PRICE\"].values\n",
    "    list_quant = [np.percentile(list_prices, quant) for quant in list_quant_name]\n",
    "\n",
    "    for index, (quant_name, quant) in enumerate(zip(list_quant_name, list_quant)) :\n",
    "        if index>0 :\n",
    "            cpltr[\"CAT_%sto%s\" % (quant_name-quant_step, quant_name)] = cpltr[\"CATALOG_PRICE\"].apply(binarize_function, val_low = list_quant[index-1], val_up = quant)\n",
    "            cplte[\"CAT_%sto%s\" % (quant_name-quant_step, quant_name)] = cplte[\"CATALOG_PRICE\"].apply(binarize_function, val_low = list_quant[index-1], val_up = quant)\n",
    "\n",
    "    list_prices = cpltr[\"DISCOUNT_PRICE\"].values\n",
    "    list_quant = [np.percentile(list_prices, quant) for quant in list_quant_name]\n",
    "    for index, (quant_name, quant) in enumerate(zip(list_quant_name, list_quant)) :\n",
    "        if index>0 :\n",
    "            cpltr[\"DIS_%sto%s\" % (quant_name-quant_step, quant_name)] = cpltr[\"DISCOUNT_PRICE\"].apply(binarize_function, val_low = list_quant[index-1], val_up = quant)\n",
    "            cplte[\"DIS_%sto%s\" % (quant_name-quant_step, quant_name)] = cplte[\"DISCOUNT_PRICE\"].apply(binarize_function, val_low = list_quant[index-1], val_up = quant)\n",
    "\n",
    "    list_col_bin = [col for col in cplte.columns.values if \"to\" in col]\n",
    "\n",
    "    # List of features\n",
    "    list_feat = [\"GENRE_NAME\", \"large_area_name\", \"small_area_name\", \"VALIDPERIOD\", \"USABLE_DATE_MON\", \"USABLE_DATE_TUE\",\n",
    "        \"USABLE_DATE_WED\", \"USABLE_DATE_THU\", \"USABLE_DATE_FRI\",\n",
    "        \"USABLE_DATE_SAT\", \"USABLE_DATE_SUN\", \"USABLE_DATE_HOLIDAY\",\n",
    "        \"USABLE_DATE_BEFORE_HOLIDAY\"] + list_col_bin\n",
    "\n",
    "    #NA imputation\n",
    "    cplte = cplte.fillna(-1)\n",
    "    cpltr = cpltr.fillna(-1)\n",
    "\n",
    "    list_col_to_str = [\"PRICE_RATE\", \"CATALOG_PRICE\", \"DISCOUNT_PRICE\", \"DISPFROM_day\", \"DISPEND_day\", \"DISPPERIOD\", \"VALIDPERIOD\"]\n",
    "    cpltr[list_col_to_str] = cpltr[list_col_to_str].astype(str)\n",
    "    cplte[list_col_to_str] = cplte[list_col_to_str].astype(str)\n",
    "\n",
    "    # Reduce dataset to features of interest\n",
    "    cpltr = cpltr[list_feat]\n",
    "    cplte = cplte[list_feat]\n",
    "\n",
    "    list_us = [col for col in list_feat if \"USABLE\" in col]\n",
    "    for col in list_us :\n",
    "        cpltr.loc[cpltr[col]>0, col] = 1\n",
    "        cpltr.loc[cpltr[col]<0, col] = 0\n",
    "        cplte.loc[cpltr[col]>0, col] = 1\n",
    "        cplte.loc[cpltr[col]<0, col] = 0\n",
    "\n",
    "    # Binarize categorical features\n",
    "    cpltr =  cpltr.T.to_dict().values()\n",
    "    vec   = DictVectorizer(sparse = True)\n",
    "    cpltr = vec.fit_transform(cpltr)\n",
    "    cplte = vec.transform(cplte.T.to_dict().values())\n",
    "\n",
    "    cplte = sps.csr_matrix(cplte, dtype = np.int32)\n",
    "    cpltr = sps.csr_matrix(cpltr, dtype = np.int32)\n",
    "\n",
    "    #Save the matrix. They are already in csr format\n",
    "    spi.mmwrite(\"../Data/Validation/%s/train_item_feat_mtrx_%s.mtx\" % (week_ID, week_ID), cpltr)\n",
    "    spi.mmwrite(\"../Data/Validation/%s/test_item_feat_mtrx_%s.mtx\" % (week_ID, week_ID), cplte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of implicit feed back user/item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_user_item_mtrx(week_ID):\n",
    "    \"\"\" Build user item matrix (for test and train datasets)\n",
    "    (sparse matrix, Mui[u,i] = 1 if user u has purchase item i, 0 otherwise)\n",
    "    \n",
    "    arg : week_ID (str) validation week\n",
    "    \"\"\"\n",
    "\n",
    "    print \"Creating user_item matrix for LightFM\"\n",
    "\n",
    "    #For now, only consider the detail dataset\n",
    "    cpdtr = pd.read_csv(\"../Data/Validation/%s/coupon_detail_train_validation_%s.csv\" % (week_ID, week_ID))\n",
    "    cpltr = pd.read_csv(\"../Data/Validation/%s/coupon_list_train_validation_%s.csv\" % (week_ID, week_ID))\n",
    "    cplte = pd.read_csv(\"../Data/Validation/%s/coupon_list_test_validation_%s.csv\" % (week_ID, week_ID))\n",
    "    ulist = pd.read_csv(\"../Data/Validation/%s/user_list_validation_%s.csv\" % (week_ID, week_ID))\n",
    "\n",
    "    #Build a dict with the coupon index in cpltr \n",
    "    d_ci_tr = {}\n",
    "    for i in range(len(cpltr)) :\n",
    "        coupon = cpltr[\"COUPON_ID_hash\"].values[i]\n",
    "        d_ci_tr[coupon] = i\n",
    "\n",
    "    #Build a dict with the user index in ulist \n",
    "    d_ui = {}       \n",
    "    for i in range(len(ulist)) :\n",
    "        user       = ulist[\"USER_ID_hash\"].values[i]\n",
    "        d_ui[user] = i\n",
    "    \n",
    "    #Build the user x item matrices using scipy lil_matrix\n",
    "    Mui_tr = sps.lil_matrix((len(ulist), len(cpltr)), dtype=np.int8)\n",
    "\n",
    "    #Now fill Mui_tr with the info from cpdtr\n",
    "    for i in range(len(cpdtr)) :\n",
    "        sys.stdout.write(\"\\rProcessing row \" + str(i)+\"/ \"+str(cpdtr.shape[0]))\n",
    "        sys.stdout.flush()\n",
    "        user        = cpdtr[\"USER_ID_hash\"].values[i]\n",
    "        coupon      = cpdtr[\"COUPON_ID_hash\"].values[i]\n",
    "        ui, ci      = d_ui[user], d_ci_tr[coupon]\n",
    "        Mui_tr[ui, ci] = 1\n",
    "    print\n",
    "\n",
    "    #Save the matrix in the COO format\n",
    "    spi.mmwrite(\"../Data/Validation/%s/user_item_train_mtrx_%s.mtx\" % (week_ID, week_ID), Mui_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inline documentation of these functions is self-explanatory. We basically specify which features we want to take into account for either users or items and we consider an implicit feedback-only case where only purchases are counted as interactions.\n",
    "\n",
    "We can now create these matrices :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build_user_item_mtrx(\"week52\")\n",
    "build_user_feature_matrix(\"week52\")\n",
    "build_item_feature_matrix(\"week52\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, let's check the matrices were created :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "week_ID = \"week52\"\n",
    "assert(os.path.isfile(\"../Data/Validation/%s/user_item_train_mtrx_%s.mtx\" % (week_ID, week_ID)))\n",
    "assert(os.path.isfile(\"../Data/Validation/%s/train_item_feat_mtrx_%s.mtx\" % (week_ID, week_ID)))\n",
    "assert(os.path.isfile(\"../Data/Validation/%s/test_item_feat_mtrx_%s.mtx\" % (week_ID, week_ID)))\n",
    "assert(os.path.isfile(\"../Data/Validation/%s/user_feat_mtrx_%s.mtx\" % (week_ID, week_ID)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright !\n",
    "Let us now load these matrices :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "week_ID = \"week52\"\n",
    "Mui_train = spi.mmread(\"../Data/Validation/%s/user_item_train_mtrx_%s.mtx\" % (week_ID, week_ID))\n",
    "uf        = spi.mmread(\"../Data/Validation/%s/user_feat_mtrx_%s.mtx\" % (week_ID, week_ID))\n",
    "itrf      = spi.mmread(\"../Data/Validation/%s/train_item_feat_mtrx_%s.mtx\" % (week_ID, week_ID))\n",
    "itef      = spi.mmread(\"../Data/Validation/%s/test_item_feat_mtrx_%s.mtx\" % (week_ID, week_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import lightfm and our evaluation metric (MAP@10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "import mean_average_precision as mapr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a function to fit a lightFM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_model(week_ID, no_comp, lr, ep):\n",
    "\t\"\"\" Fit the lightFM model to all weeks in list_week_ID.\n",
    "\tThen print the results for MAPat10\n",
    "\t\n",
    "\targs : week_ID validation test week\n",
    "\tno_comp, lr, ep = (int, float, int) number of components, learning rate, number of epochs for lightFM model\n",
    "\n",
    "    returns: d_user_pred, list_user, list_coupon\n",
    "    list_coupon = list of test coupons \n",
    "    list_user = list of user ID \n",
    "    d_user_pred : key = user, value = predicted ranking of coupons in list_coupon\n",
    "\n",
    "\t\"\"\"\n",
    "\n",
    "\tprint \"Fit lightfm model for %s\" % week_ID\n",
    "\n",
    "\t#Load data\n",
    "\tMui_train = spi.mmread(\"../Data/Validation/%s/user_item_train_mtrx_%s.mtx\" % (week_ID, week_ID))\n",
    "\tuf        = spi.mmread(\"../Data/Validation/%s/user_feat_mtrx_%s.mtx\" % (week_ID, week_ID))\n",
    "\titrf      = spi.mmread(\"../Data/Validation/%s/train_item_feat_mtrx_%s.mtx\" % (week_ID, week_ID))\n",
    "\titef      = spi.mmread(\"../Data/Validation/%s/test_item_feat_mtrx_%s.mtx\" % (week_ID, week_ID))\n",
    "\n",
    "\t#Print shapes as a check\n",
    "\tprint \"user_features shape: %s,\\nitem train features shape: %s,\\nitem test features shape: %s\"   % (uf.shape, itrf.shape, itef.shape)\n",
    "\n",
    "\t#Load test coupon  and user lists\n",
    "\tcplte       = pd.read_csv(\"../Data/Validation/\" + week_ID + \"/coupon_list_test_validation_\" + week_ID +\".csv\")\n",
    "\tulist       = pd.read_csv(\"../Data/Validation/\" + week_ID + \"/user_list_validation_\" + week_ID +\".csv\")\n",
    "\tlist_coupon = cplte[\"COUPON_ID_hash\"].values\n",
    "\tlist_user   = ulist[\"USER_ID_hash\"].values\n",
    "\n",
    "\t#Build model\n",
    "\tmodel = LightFM(no_components=no_comp, learning_rate=lr, loss='warp')\n",
    "\tmodel.fit_partial(Mui_train, user_features = uf, item_features = itrf, epochs = ep, num_threads = 4, verbose = True)\n",
    "\n",
    "\ttest               = sps.csr_matrix((len(list_user), len(list_coupon)), dtype = np.int32)\n",
    "\tno_users, no_items = test.shape\n",
    "\tpid_array          = np.arange(no_items, dtype=np.int32)\n",
    "\n",
    "\t#Create and initialise dict to store predictions\n",
    "\td_user_pred = {}\n",
    "\tfor user in list_user :\n",
    "\t\td_user_pred[user] = []\n",
    "\t\n",
    "\t# Loop over users and compute predictions\n",
    "\tfor user_id, row in enumerate(test):\n",
    "\t\tsys.stdout.write(\"\\rProcessing user \" + str(user_id)+\"/ \"+str(len(list_user)))\n",
    "\t\tsys.stdout.flush()\n",
    "\t\tuid_array         = np.empty(no_items, dtype=np.int32)\n",
    "\t\tuid_array.fill(user_id)\n",
    "\t\tpredictions       = model.predict(uid_array, pid_array,user_features = uf, item_features = itef, num_threads=4)\n",
    "\t\tuser              = str(list_user[user_id])\n",
    "\t\td_user_pred[user] = predictions\n",
    "\n",
    "\treturn d_user_pred, list_user, list_coupon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a scoring function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_lightFM(no_comp, lr, ep):\n",
    "\t\"\"\"\n",
    "\tScore the lightFM model for mean average precision at k = 10\n",
    "\n",
    "\targs = no_comp, lr, ep (int, float, int) \n",
    "\tnumber of components, learning rate, number of epochs for lightFM model\n",
    "\t\"\"\"\n",
    "       \n",
    "\tlist_score = []\n",
    "\n",
    "\t# Loop over validation weeks\n",
    "\tfor week_ID in [\"week52\"] :\n",
    "\t\t#Get predictions, manually choose metric and classifier\n",
    "\t\td_user_pred, list_user_full, list_coupon   = fit_model(week_ID, no_comp, lr, ep)\n",
    "\t\t#Format predictions\n",
    "\t\tfor index, user in enumerate(list_user_full) :\n",
    "\t\t\tlist_pred         = d_user_pred[user]\n",
    "\t\t\ttop_k             = np.argsort(-list_pred)[:10]\n",
    "\t\t\td_user_pred[user] = list_coupon[top_k]\n",
    "\t\t\n",
    "\t\t#Get actual purchase\n",
    "\t\td_user_purchase = {}\n",
    "\t\twith open(\"../Data/Validation/\" + week_ID + \"/dict_purchase_validation_\" + week_ID + \".pickle\", \"r\") as fp:\n",
    "\t\t\td_user_purchase = pickle.load(fp)\n",
    "\t\t\n",
    "\t\t# Take care of users who registered during validation test week\n",
    "\t\tfor key in d_user_purchase.keys() :\n",
    "\t\t\ttry :\n",
    "\t\t\t\td_user_pred[key]\n",
    "\t\t\texcept KeyError :\n",
    "\t\t\t\td_user_pred[key] = []\n",
    "\t\t\n",
    "\t\tlist_user = d_user_purchase.keys()\n",
    "\t\tlist_actual = [d_user_purchase[key] for key in list_user]\n",
    "\t\tlist_pred = [d_user_pred[key] for key in list_user] \n",
    "\n",
    "        list_score.append(mapr.mapk(list_actual, list_pred))\n",
    "        print list_score\n",
    "\n",
    "\tlist_score = np.array(list_score)\n",
    "\tprint list_score \n",
    "\tprint str(np.mean(list_score)) + \" +/- \" + str(np.std(list_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate our lightFM model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_comp, lr, ep = 10, 0.01, 10 #10 components, 0.01 learning rate, 10 epochs\n",
    "score_lightFM(no_comp, lr, ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
